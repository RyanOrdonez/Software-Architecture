{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DTSA 5508: Software Architecture Patterns for Big Data\n",
    "\n",
    "## Course Overview and Quick Reference Guide\n",
    "\n",
    "This notebook serves as a comprehensive overview and quick reference guide for the key concepts, techniques, and implementations covered in this course.\n",
    "\n",
    "### Course Objectives\n",
    "- Understanding big data architecture patterns\n",
    "- Implementing distributed systems\n",
    "- Working with data processing frameworks\n",
    "- Applying scalable design patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Import common libraries\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Display settings\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn')\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Week 1: Introduction to Big Data Architecture\n",
    "\n",
    "### Key Concepts\n",
    "- \n",
    "\n",
    "### Important Terms\n",
    "- \n",
    "\n",
    "### Code Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def initialize_spark_session():\n",
    "    \"\"\"Initialize a Spark session for big data processing\"\"\"\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"BigDataProcessing\") \\\n",
    "        .config(\"spark.memory.offHeap.enabled\", True) \\\n",
    "        .config(\"spark.memory.offHeap.size\", \"10g\") \\\n",
    "        .getOrCreate()\n",
    "    \n",
    "    print(\"Spark Configuration:\")\n",
    "    print(f\"Spark Version: {spark.version}\")\n",
    "    print(f\"Master: {spark.sparkContext.master}\")\n",
    "    print(f\"Application ID: {spark.sparkContext.applicationId}\")\n",
    "    \n",
    "    return spark\n",
    "\n",
    "def demonstrate_distributed_processing(spark, data_path):\n",
    "    \"\"\"Demonstrate distributed data processing\"\"\"\n",
    "    # Read data\n",
    "    df = spark.read.csv(data_path, header=True, inferSchema=True)\n",
    "    \n",
    "    # Show basic info\n",
    "    print(\"\\nDataset Info:\")\n",
    "    print(f\"Number of partitions: {df.rdd.getNumPartitions()}\")\n",
    "    print(f\"Number of records: {df.count()}\")\n",
    "    \n",
    "    # Show schema\n",
    "    print(\"\\nSchema:\")\n",
    "    df.printSchema()\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Week 2: Data Processing Patterns\n",
    "\n",
    "### Key Concepts\n",
    "- \n",
    "\n",
    "### Important Patterns\n",
    "- \n",
    "\n",
    "### Code Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def implement_data_pipeline(spark, input_data):\n",
    "    \"\"\"Implement a typical big data processing pipeline\"\"\"\n",
    "    # 1. Data ingestion\n",
    "    df = spark.createDataFrame(input_data)\n",
    "    \n",
    "    # 2. Data transformation\n",
    "    transformed_df = df.withColumn(\"processed_date\", F.current_date()) \\\n",
    "        .withColumn(\"value_squared\", F.col(\"value\") * F.col(\"value\")) \\\n",
    "        .withWatermark(\"timestamp\", \"10 minutes\")\n",
    "    \n",
    "    # 3. Data aggregation\n",
    "    aggregated_df = transformed_df.groupBy(\"category\") \\\n",
    "        .agg(F.avg(\"value\").alias(\"avg_value\"),\n",
    "             F.count(\"*\").alias(\"count\"))\n",
    "    \n",
    "    # 4. Show results\n",
    "    print(\"Pipeline Results:\")\n",
    "    aggregated_df.show()\n",
    "    \n",
    "    return aggregated_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Week 3: Scalable System Design\n",
    "\n",
    "### Key Concepts\n",
    "- \n",
    "\n",
    "### Important Components\n",
    "- \n",
    "\n",
    "### Code Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "class DataProcessor:\n",
    "    \"\"\"Example of a scalable data processor\"\"\"\n",
    "    def __init__(self, spark_session, batch_size=1000):\n",
    "        self.spark = spark_session\n",
    "        self.batch_size = batch_size\n",
    "    \n",
    "    def process_in_batches(self, data):\n",
    "        \"\"\"Process data in batches\"\"\"\n",
    "        total_batches = (len(data) + self.batch_size - 1) // self.batch_size\n",
    "        results = []\n",
    "        \n",
    "        for i in range(total_batches):\n",
    "            start_idx = i * self.batch_size\n",
    "            end_idx = min((i + 1) * self.batch_size, len(data))\n",
    "            batch = data[start_idx:end_idx]\n",
    "            \n",
    "            # Process batch\n",
    "            batch_df = self.spark.createDataFrame(batch)\n",
    "            processed = self._process_batch(batch_df)\n",
    "            results.append(processed)\n",
    "        \n",
    "        return self.spark.union(results)\n",
    "    \n",
    "    def _process_batch(self, batch_df):\n",
    "        \"\"\"Process a single batch\"\"\"\n",
    "        return batch_df.withColumn(\"processed\", F.lit(True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Week 4: System Integration and Deployment\n",
    "\n",
    "### Key Concepts\n",
    "- \n",
    "\n",
    "### Important Practices\n",
    "- \n",
    "\n",
    "### Code Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def deploy_streaming_pipeline(spark):\n",
    "    \"\"\"Example of a streaming data pipeline\"\"\"\n",
    "    # Create streaming DataFrame\n",
    "    streaming_df = spark.readStream \\\n",
    "        .format(\"kafka\") \\\n",
    "        .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n",
    "        .option(\"subscribe\", \"input-topic\") \\\n",
    "        .load()\n",
    "    \n",
    "    # Process streaming data\n",
    "    processed_df = streaming_df \\\n",
    "        .select(F.from_json(F.col(\"value\").cast(\"string\"), schema).alias(\"data\")) \\\n",
    "        .select(\"data.*\") \\\n",
    "        .withWatermark(\"timestamp\", \"1 minute\") \\\n",
    "        .groupBy(F.window(\"timestamp\", \"5 minutes\"), \"category\") \\\n",
    "        .agg(F.count(\"*\").alias(\"count\"))\n",
    "    \n",
    "    # Write stream\n",
    "    query = processed_df.writeStream \\\n",
    "        .outputMode(\"complete\") \\\n",
    "        .format(\"console\") \\\n",
    "        .start()\n",
    "    \n",
    "    return query"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
