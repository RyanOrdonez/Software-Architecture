{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DTSA 5022: Trees, SVM, and Unsupervised Learning\n",
    "\n",
    "## Course Overview and Quick Reference Guide\n",
    "\n",
    "This notebook serves as a comprehensive overview and quick reference guide for the key concepts, techniques, and implementations covered in this course.\n",
    "\n",
    "### Course Objectives\n",
    "- Understanding decision trees and ensemble methods\n",
    "- Implementing Support Vector Machines\n",
    "- Working with unsupervised learning algorithms\n",
    "- Applying clustering and dimensionality reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Import common libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Display settings\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn')\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Week 1: Decision Trees and Random Forests\n",
    "\n",
    "### Key Concepts\n",
    "- \n",
    "\n",
    "### Important Terms\n",
    "- \n",
    "\n",
    "### Code Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def train_and_visualize_tree(X, y, max_depth=3):\n",
    "    \"\"\"Train and visualize a decision tree\"\"\"\n",
    "    # Train decision tree\n",
    "    tree = DecisionTreeClassifier(max_depth=max_depth, random_state=42)\n",
    "    tree.fit(X, y)\n",
    "    \n",
    "    # Visualize tree\n",
    "    plt.figure(figsize=(20,10))\n",
    "    plot_tree(tree, feature_names=X.columns, class_names=np.unique(y).astype(str),\n",
    "              filled=True, rounded=True)\n",
    "    plt.title(f'Decision Tree (max_depth={max_depth})')\n",
    "    plt.show()\n",
    "    \n",
    "    # Feature importance\n",
    "    importance = pd.DataFrame({\n",
    "        'feature': X.columns,\n",
    "        'importance': tree.feature_importances_\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.barplot(data=importance, x='importance', y='feature')\n",
    "    plt.title('Feature Importance')\n",
    "    plt.show()\n",
    "    \n",
    "    return tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Week 2: Support Vector Machines\n",
    "\n",
    "### Key Concepts\n",
    "- \n",
    "\n",
    "### Important Components\n",
    "- \n",
    "\n",
    "### Code Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def train_and_visualize_svm(X, y, kernel='rbf'):\n",
    "    \"\"\"Train and visualize SVM with different kernels\"\"\"\n",
    "    # Train SVM\n",
    "    svm = SVC(kernel=kernel, random_state=42)\n",
    "    svm.fit(X, y)\n",
    "    \n",
    "    # Create mesh grid for visualization\n",
    "    x_min, x_max = X.iloc[:, 0].min() - 1, X.iloc[:, 0].max() + 1\n",
    "    y_min, y_max = X.iloc[:, 1].min() - 1, X.iloc[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.02),\n",
    "                         np.arange(y_min, y_max, 0.02))\n",
    "    \n",
    "    # Plot decision boundary\n",
    "    Z = svm.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.contourf(xx, yy, Z, alpha=0.4)\n",
    "    plt.scatter(X.iloc[:, 0], X.iloc[:, 1], c=y, alpha=0.8)\n",
    "    plt.title(f'SVM Decision Boundary (kernel={kernel})')\n",
    "    plt.show()\n",
    "    \n",
    "    return svm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Week 3: Clustering Algorithms\n",
    "\n",
    "### Key Concepts\n",
    "- \n",
    "\n",
    "### Important Methods\n",
    "- \n",
    "\n",
    "### Code Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def perform_clustering(X, n_clusters=3):\n",
    "    \"\"\"Perform and visualize K-means clustering\"\"\"\n",
    "    # Perform K-means\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "    clusters = kmeans.fit_predict(X)\n",
    "    \n",
    "    # Visualize clusters\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    scatter = plt.scatter(X.iloc[:, 0], X.iloc[:, 1], c=clusters, cmap='viridis')\n",
    "    plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1],\n",
    "                marker='x', s=200, linewidths=3, color='r', label='Centroids')\n",
    "    plt.title(f'K-means Clustering (k={n_clusters})')\n",
    "    plt.colorbar(scatter)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    # Elbow method\n",
    "    inertias = []\n",
    "    k_range = range(1, 11)\n",
    "    for k in k_range:\n",
    "        kmeans_temp = KMeans(n_clusters=k, random_state=42)\n",
    "        kmeans_temp.fit(X)\n",
    "        inertias.append(kmeans_temp.inertia_)\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(k_range, inertias, 'bx-')\n",
    "    plt.title('Elbow Method')\n",
    "    plt.xlabel('k')\n",
    "    plt.ylabel('Inertia')\n",
    "    plt.show()\n",
    "    \n",
    "    return kmeans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Week 4: Dimensionality Reduction\n",
    "\n",
    "### Key Concepts\n",
    "- \n",
    "\n",
    "### Important Techniques\n",
    "- \n",
    "\n",
    "### Code Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def perform_pca(X, n_components=2):\n",
    "    \"\"\"Perform and visualize PCA\"\"\"\n",
    "    # Perform PCA\n",
    "    pca = PCA(n_components=n_components)\n",
    "    X_pca = pca.fit_transform(X)\n",
    "    \n",
    "    # Explained variance ratio\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(range(1, len(pca.explained_variance_ratio_) + 1),\n",
    "             np.cumsum(pca.explained_variance_ratio_), 'bo-')\n",
    "    plt.title('Cumulative Explained Variance Ratio')\n",
    "    plt.xlabel('Number of Components')\n",
    "    plt.ylabel('Cumulative Explained Variance')\n",
    "    plt.show()\n",
    "    \n",
    "    # Component weights\n",
    "    components = pd.DataFrame(\n",
    "        pca.components_,\n",
    "        columns=X.columns,\n",
    "        index=[f'PC{i+1}' for i in range(n_components)]\n",
    "    )\n",
    "    \n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.heatmap(components, annot=True, cmap='coolwarm', center=0)\n",
    "    plt.title('PCA Components')\n",
    "    plt.show()\n",
    "    \n",
    "    return pca, X_pca"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
